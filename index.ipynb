{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94ba807a",
   "metadata": {},
   "source": [
    "# House Sales Analysis in NorthWestern county"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586146da",
   "metadata": {},
   "source": [
    "## 1. Business Understanding\n",
    "### a) Introduction\n",
    "\n",
    "House sales began in 1890s in the United States and since then its been growing all over the world and agencies started to form to enhance and ease the house selling process.Last year the revenue was estimated to be $4.25M with prospects of growth as time goes by. House sales are mainly influenced by the number of bedrooms, bathrooms, the year built, square footage and whether renovations are done or not among other factors.\n",
    "\n",
    "In this case the Northwest agencies aim to address the need of providing homeowners with accurate and actionable advice on how home renovations can potentially increase the estimated value of their properties and by what amount. By understanding the relationship between various renovation factors and house prices, the agencies can be able to guide homeowners in making informed decisions about renovations, which will ultimately lead to maximization of return on their investment which will enable them sell their homes at optimal prices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76fc9d1",
   "metadata": {},
   "source": [
    "### b) Problem statement\n",
    "\n",
    "The real estate industry faces the challenge of providing homeowners with reliable information about how various home renovation factors impact the estimated value of their homes. Our project addresses this problem by utilizing data analysis and regression modeling to identify key factors that affect house prices in a northwestern county. By understanding these factors, we can provide recommendations and insights to stakeholders on how to effectively advise homeowners on renovations that can potentially increase the value of their properties."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db766d2",
   "metadata": {},
   "source": [
    "### c) Main Objective\n",
    "\n",
    "The main objective of this project is to develop a predictive model that estimates house prices based on various features such as the number of bedrooms and bathrooms, square footage, and year built. By building a robust regression model, we aim to accurately predict house prices and provide stakeholders with valuable insights into the factors driving price fluctuations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e73317e",
   "metadata": {},
   "source": [
    "### d) Metric of success\n",
    "\n",
    "The success of our project will be evaluated based on the model's performance in predicting house prices. We will use evaluation metrics such as the coefficient of determination (R-squared) and root mean square error (RMSE) to assess the model's accuracy. A higher R-squared value and lower RMSE indicate a more successful model in capturing the variations in house prices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89fb339",
   "metadata": {},
   "source": [
    "### e) Specific Objectives\n",
    "\n",
    "- Explore and preprocess the King County House Sales dataset, including handling missing values, transforming features, and encoding categorical variables.\n",
    "- Perform exploratory data analysis to gain insights into the distribution and relationships between different features and the target variable.\n",
    "- Conduct feature selection to identify the most influential factors that affect house prices and eliminate irrelevant or redundant features.\n",
    "- Build multiple linear regression models with different combinations of features and evaluate their performance using appropriate metrics.\n",
    "- Interpret the results of the final regression model, including the coefficients of the selected features and their implications on house prices.\n",
    "- Provide recommendations to stakeholders based on the insights gained from the modeling process, suggesting specific renovation factors that homeowners can focus on to increase the estimated value of their properties."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace55e70",
   "metadata": {},
   "source": [
    "##  2. Data Understanding\n",
    "\n",
    "We will be utilizing the King County House Sales datasets which contains information on house sales, including features such as price, number of bedrooms and bathrooms, square footage, and year built. By thoroughly understanding and analyzing the dataset and its column descriptions, we can identify the relevant features to include in our analysis and modeling process\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70002bba",
   "metadata": {},
   "source": [
    "## 3. Data Preparation\n",
    "\n",
    "This process involves cleaning, transforming, and organizing the data to ensure its suitability for analysis and modeling. \n",
    "\n",
    "- Importing relevant libraries\n",
    "- Loading the dataset and checking it contains\n",
    "- Dealing with missing data\n",
    "- Checking and removing duplicates\n",
    "- Handling outliers\n",
    "- Feature scaling and normalization using z-scores\n",
    "- Encoding categoriacl variables using one-hot encoding\n",
    "- Exploring the dataset to identify opportunities for creating new features that may enhance the predictive power of the model \n",
    "- Splitting the dataset into training and test sets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567d83ca",
   "metadata": {},
   "source": [
    "### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f96aec48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc3c243",
   "metadata": {},
   "source": [
    "### Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "23002af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the house dataset and previewing the first five outputs\n",
    "data = pd.read_csv(\"data/kc_house_data.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f21a2571",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# getting an overview of the dataset, including the number of non-null values and the data types of each column\n",
    "print(data.info())\n",
    "# getting the number of rows and columns in the data.\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8db534d",
   "metadata": {},
   "source": [
    "The dataset has 21597 rows and 21 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d920ee51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the datatypes of each columns\n",
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7851513d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating summary statistics for the numerical columns in the dataset\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c81a34",
   "metadata": {},
   "source": [
    "Most houses have an average of 3 bedrooms,2 bathrooms and were built between 1970 and 2015"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e05ac2",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b09c126",
   "metadata": {},
   "source": [
    "### Checking for missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01b1a365",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the proportion of missing values per column\n",
    "data.isna().sum()/len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33806539",
   "metadata": {},
   "source": [
    "Three columns have missing values but we will only work with waterfront since it will be used in analysis and modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4520cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# since waterfront has missing values, we first check the value counts\n",
    "data['waterfront'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce2a00ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filling in missing values in the 'waterfront' column with the string 'NO'\n",
    "data['waterfront'] = data['waterfront'].fillna('NO')\n",
    "# getting an overview of the dataset after filling the missing values in Waterfront\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf9011e",
   "metadata": {},
   "source": [
    "We filled the missing values in waterfront instead of removing them since we want each column to have the same number of rows.\n",
    "we can now see that the waterfront column has the same values as the other columns we will be using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1f04167f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping columns that we will not be using in data analysis.\n",
    "columns_to_drop = ['date', 'view', 'sqft_above', 'sqft_basement', 'yr_renovated', 'zipcode', 'lat', 'long', 'sqft_living15', 'sqft_lot15']\n",
    "data.drop(columns_to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077f8a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking to see if the columns have been dropped.\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95e7961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking if there are any duplicates\n",
    "data.duplicated().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33949b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the sum of duplicated ids\n",
    "data['id'].duplicated().sum()\n",
    "# dropping the duplicated values and keeping the first in id column\n",
    "data = data.drop_duplicates(subset='id', keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aabbdc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now checking the if there are any duplicated values\n",
    "data.duplicated().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6635cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the rows and columns after dropping duplicates and unwanted columns.\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18510a84",
   "metadata": {},
   "source": [
    "## Explotory data analysis\n",
    "\n",
    "### Univariate analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74700120",
   "metadata": {},
   "source": [
    "### Dealing with outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2d34a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting the numerical columns in the dataset\n",
    "numeric_columns = ['price', 'bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'yr_built']\n",
    "\n",
    "# Calculating z-scores for the numerical columns\n",
    "z_scores = (data[numeric_columns] - data[numeric_columns].mean()) / data[numeric_columns].std()\n",
    "\n",
    "# defining a threshold for identifying outliers\n",
    "threshold = 3\n",
    "\n",
    "# Find the indices of outliers for each column\n",
    "outlier_indices = (z_scores > threshold).any(axis=1)\n",
    "\n",
    "# Extract the outlier rows from the dataset\n",
    "outliers = data[outlier_indices]\n",
    "\n",
    "# Plotting the outliers\n",
    "for column in numeric_columns:\n",
    "    plt.figure()\n",
    "    plt.boxplot(data[column])\n",
    "    plt.title(column)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b81f77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the outlier rows from the dataset\n",
    "data_cleaned = data[~outlier_indices]\n",
    "\n",
    "# Reset the index of the cleaned dataset\n",
    "data_cleaned.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Plot histograms of the cleaned dataset\n",
    "data_cleaned[numeric_columns].hist(bins=20, figsize=(10, 8))\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot scatterplots of price against other numeric columns\n",
    "sns.pairplot(data_cleaned, x_vars=['bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot'], y_vars='price', height=5)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5a456e",
   "metadata": {},
   "source": [
    " ## Data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102f51ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4629becb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking for categorical columns\n",
    "categorical_columns = data.select_dtypes(include=['object', 'category']).columns\n",
    "categorical_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bac0fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exploring categorical variables,using value_counts() to get the count of each unique value in a column \n",
    "# using describe() to get summary statistics for categorical columns.\n",
    "for column in categorical_columns:\n",
    "    value_counts = data[column].value_counts()\n",
    "    print(f\"Value counts for {column}:\\n{value_counts}\\n\")\n",
    "\n",
    "# Summary statistics for categorical columns\n",
    "print(data[categorical_columns].describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82363df8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Correlation analysis\n",
    "correlation_matrix = data_cleaned[numeric_columns].corr()\n",
    "print(\"\\nCorrelation Matrix:\")\n",
    "print(correlation_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fcc7cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')\n",
    "plt.title('Correlation Between Columns')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbaf1db",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# one hot encoding\n",
    "data_encoded = pd.get_dummies(data, columns=categorical_columns, drop_first=True)\n",
    "data_encoded.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d5f76d",
   "metadata": {},
   "source": [
    "## regression modelling\n",
    "\n",
    " To perform regression modeling, we will split the dataset into features (X) and target variable (y), and then split them into training and testing sets. We will use the LinearRegression model from scikit-learn library to build the regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d01dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression Modeling\n",
    "\n",
    "# Splitting the data into features (X) and target variable (y)\n",
    "X = data_cleaned[numeric_columns[1:]]\n",
    "y = data_cleaned[numeric_columns[0]]\n",
    "\n",
    "# Splitting the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Creating and fitting the linear regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Making predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Model evaluation\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"\\nRegression Model Evaluation:\")\n",
    "print(f\"Mean Squared Error: {mse:.2f}\")\n",
    "print(f\"R-squared: {r2:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b480eb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Basic Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a256fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 1: Basic Linear Regression\n",
    "X = data_cleaned[numeric_columns[1:]]\n",
    "y = data_cleaned[numeric_columns[0]]\n",
    "\n",
    "# Splitting the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Creating and fitting the linear regression model\n",
    "model1 = LinearRegression()\n",
    "model1.fit(X_train, y_train)\n",
    "\n",
    "# Making predictions on the test set\n",
    "y_pred = model1.predict(X_test)\n",
    "\n",
    "# Model evaluation\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"Model 1: Basic Linear Regression\")\n",
    "print(f\"Mean Squared Error: {mse:.2f}\")\n",
    "print(f\"R-squared: {r2:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450ac49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 2: Feature Selection\n",
    "# Selecting features with correlation greater than a threshold\n",
    "correlation_threshold = 0.8\n",
    "selected_features = correlation_matrix['price'][correlation_matrix['price'].abs() > correlation_threshold].index.tolist()\n",
    "\n",
    "X = data_cleaned[selected_features]\n",
    "y = data_cleaned[numeric_columns[0]]\n",
    "\n",
    "# Splitting the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Creating and fitting the linear regression model\n",
    "model2 = LinearRegression()\n",
    "model2.fit(X_train, y_train)\n",
    "\n",
    "# Making predictions on the test set\n",
    "y_pred = model2.predict(X_test)\n",
    "\n",
    "# Model evaluation\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"\\nModel 2: Feature Selection\")\n",
    "print(f\"Selected Features: {selected_features}\")\n",
    "print(f\"Mean Squared Error: {mse:.2f}\")\n",
    "print(f\"R-squared: {r2:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25f2792",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Polynomial Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8d3359",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = data_cleaned[numeric_columns[1:]]\n",
    "y = data_cleaned[numeric_columns[0]]\n",
    "\n",
    "# Splitting the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Creating polynomial features\n",
    "poly_features = PolynomialFeatures(degree=2)\n",
    "X_train_poly = poly_features.fit_transform(X_train)\n",
    "X_test_poly = poly_features.transform(X_test)\n",
    "\n",
    "# Creating and fitting the linear regression model with polynomial features\n",
    "model3 = LinearRegression()\n",
    "model3.fit(X_train_poly, y_train)\n",
    "\n",
    "# Making predictions on the test set\n",
    "y_pred = model3.predict(X_test_poly)\n",
    "\n",
    "# Model evaluation\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"\\nModel 3: Polynomial Regression\")\n",
    "print(f\"Degree of Polynomial: {poly_features.degree}\")\n",
    "print(f\"Mean Squared Error: {mse:.2f}\")\n",
    "print(f\"R-squared: {r2:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342ff2c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
