{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94ba807a",
   "metadata": {},
   "source": [
    "# House Sales Analysis in NorthWestern county"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586146da",
   "metadata": {},
   "source": [
    "## 1. Business Understanding\n",
    "### a) Introduction\n",
    "\n",
    "House sales began in 1890s in the United States and since then its been growing all over the world and agencies started to form to enhance and ease the house selling process.Last year the revenue was estimated to be $4.25M with prospects of growth as time goes by. House sales are mainly influenced by the number of bedrooms, bathrooms, the year built, square footage and whether renovations are done or not among other factors.\n",
    "\n",
    "In this case the Northwest agencies aim to address the need of providing homeowners with accurate and actionable advice on how home renovations can potentially increase the estimated value of their properties and by what amount. By understanding the relationship between various renovation factors and house prices, the agencies can be able to guide homeowners in making informed decisions about renovations, which will ultimately lead to maximization of return on their investment which will enable them sell their homes at optimal prices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76fc9d1",
   "metadata": {},
   "source": [
    "### b) Problem statement\n",
    "\n",
    "The real estate industry faces the challenge of providing homeowners with reliable information about how various home renovation factors impact the estimated value of their homes. Our project addresses this problem by utilizing data analysis and regression modeling to identify key factors that affect house prices in a northwestern county. By understanding these factors, we can provide recommendations and insights to stakeholders on how to effectively advise homeowners on renovations that can potentially increase the value of their properties."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db766d2",
   "metadata": {},
   "source": [
    "### c) Main Objective\n",
    "\n",
    "The main objective of this project is to develop a predictive model that estimates house prices based on various features such as the number of bedrooms and bathrooms, square footage, and year built. By building a robust regression model, we aim to accurately predict house prices and provide stakeholders with valuable insights into the factors driving price fluctuations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e73317e",
   "metadata": {},
   "source": [
    "### d) Metric of success\n",
    "\n",
    "The success of our project will be evaluated based on the model's performance in predicting house prices. We will use evaluation metrics such as the coefficient of determination (R-squared) and root mean square error (RMSE) to assess the model's accuracy. A higher R-squared value and lower RMSE indicate a more successful model in capturing the variations in house prices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89fb339",
   "metadata": {},
   "source": [
    "### e) Specific Objectives\n",
    "\n",
    "- Explore and preprocess the King County House Sales dataset, including handling missing values, transforming features, and encoding categorical variables.\n",
    "- Perform exploratory data analysis to gain insights into the distribution and relationships between different features and the target variable.\n",
    "- Conduct feature selection to identify the most influential factors that affect house prices and eliminate irrelevant or redundant features.\n",
    "- Build multiple linear regression models with different combinations of features and evaluate their performance using appropriate metrics.\n",
    "- Interpret the results of the final regression model, including the coefficients of the selected features and their implications on house prices.\n",
    "- Provide recommendations to stakeholders based on the insights gained from the modeling process, suggesting specific renovation factors that homeowners can focus on to increase the estimated value of their properties."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace55e70",
   "metadata": {},
   "source": [
    "##  2. Data Understanding\n",
    "\n",
    "The data used for this project is the King County House Sales dataset. The dataset contains information on house sales in a northwestern county, including various features such as the number of bedrooms and bathrooms, square footage, location, and other relevant details.The dataset is suitable for the project as it provides comprehensive information about house sales and house features, which allows for analysis and modeling to understand the relationship between these features and the sale prices of the houses.\n",
    "\n",
    "The dataset consists of a substantial number of records, with each record representing a house sale. It includes information on multiple features, such as bedrooms, bathrooms, square footage, and more. To gain insights into the dataset, we will present descriptive statistics for all the features used in the analysis. These statistics will include measures of central tendency  and dispersion to provide an overview of the distribution and variability of the data.\n",
    "\n",
    "The features included in the analysis are selected based on their relevance and potential impact on house prices. Features such as the number of bedrooms and bathrooms, square footage, and location are commonly considered important factors affecting house prices. By including these features in the analysis, we aim to capture the significant aspects that contribute to the variation in house prices and provide valuable insights to homeowners seeking advice on home renovations.\n",
    "\n",
    "Even though this dataset provides a rich source of information, it also has limitations which include absence features that could also influence house prices  such as proximity to public transportation, missing data in certain columns, and the inherent complexity of real estate market dynamics that cannot be fully captured by the dataset alone.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70002bba",
   "metadata": {},
   "source": [
    "## 3. Data Preparation\n",
    "\n",
    "This process involves cleaning, transforming, and organizing the data to ensure its suitability for analysis and modeling. \n",
    "\n",
    "- Importing relevant libraries\n",
    "- Loading the dataset and checking it contains\n",
    "- Dealing with missing data\n",
    "- Checking and removing duplicates\n",
    "- Handling outliers\n",
    "- Feature scaling and normalization using z-scores\n",
    "- Encoding categoriacl variables using one-hot encoding\n",
    "- Exploring the dataset to identify opportunities for creating new features that may enhance the predictive power of the model \n",
    "- Splitting the dataset into training and test sets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567d83ca",
   "metadata": {},
   "source": [
    "### 3.1 Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f96aec48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc3c243",
   "metadata": {},
   "source": [
    "### 3.2 Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "23002af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the house dataset and previewing the last five outputs to check what each column contains\n",
    "data = pd.read_csv(\"data/kc_house_data.csv\")\n",
    "data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f21a2571",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# getting an overview of the dataset, including the number of non-null values and the data types of each column\n",
    "print(data.info())\n",
    "# getting the number of rows and columns in the data.\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8db534d",
   "metadata": {},
   "source": [
    "The dataset has 21597 rows and 21 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d920ee51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the datatypes of each columns\n",
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7851513d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating summary statistics for the numerical columns in the dataset\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c81a34",
   "metadata": {},
   "source": [
    "Most houses have an average of 3 bedrooms,2 bathrooms and were built between 1970 and 2015"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e05ac2",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b09c126",
   "metadata": {},
   "source": [
    "### Handling missing values\n",
    "\n",
    "We will check to see which columns have missing values and fill them using .fillna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01b1a365",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the proportion of missing values per column\n",
    "data.isna().sum()/len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33806539",
   "metadata": {},
   "source": [
    "Three columns have missing values but we will only work with waterfront since it will be used in analysis and modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4520cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# since waterfront has missing values, we first check the value counts\n",
    "data['waterfront'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce2a00ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filling in missing values in the 'waterfront' column with the string 'NO'\n",
    "data['waterfront'] = data['waterfront'].fillna('NO')\n",
    "# getting an overview of the dataset after filling the missing values in Waterfront\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf9011e",
   "metadata": {},
   "source": [
    "We filled the missing values in waterfront instead of removing them since we want each column to have the same number of rows.\n",
    "we can now see that the waterfront column has the same values as the other columns we will be using."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12095c3",
   "metadata": {},
   "source": [
    "### Dropping Columns\n",
    "\n",
    "Dropping columns that will not be used during modelling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f04167f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping columns that we will not be using in data analysis.\n",
    "columns_to_drop = ['date', 'view', 'sqft_above', 'sqft_basement', 'yr_renovated', 'zipcode', 'lat', 'long', 'sqft_living15', 'sqft_lot15']\n",
    "data.drop(columns=columns_to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "077f8a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking to see if the columns have been dropped.\n",
    "print(data.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1965fdc8",
   "metadata": {},
   "source": [
    "The columns have been dropped and we remaining with 11 columns that we will be working with."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b12c44",
   "metadata": {},
   "source": [
    "### Handling duplicates\n",
    "\n",
    "We will check if there are any duplicated values, drop them incase they are there and keep the first value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d95e7961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking if there are any duplicates\n",
    "data.duplicated().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b33949b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the sum of duplicated ids\n",
    "data['id'].duplicated().sum()\n",
    "# dropping the duplicated values and keeping the first in id column\n",
    "data = data.drop_duplicates(subset='id', keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3aabbdc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now checking the if there are any duplicated values\n",
    "data.duplicated().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fa6635cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the shape of the data to see if duplicated values have been dropped.\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c611af",
   "metadata": {},
   "source": [
    "As we can see the entry values changed from 21597 to 21420 meaning duplicated values were dropped"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74700120",
   "metadata": {},
   "source": [
    "### Handling outliers\n",
    "\n",
    "Firstly we will identify outliers, get visualizations when there are outliers, we will then go ahead and remove them and get visualizations to see the difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bb2d34a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting the numerical columns in the dataset\n",
    "numeric_columns = ['price', 'bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'yr_built', 'floors']\n",
    "\n",
    "# Calculating z-scores for the numerical columns\n",
    "z_scores = (data[numeric_columns] - data[numeric_columns].mean()) / data[numeric_columns].std()\n",
    "\n",
    "# defining a threshold for identifying outliers\n",
    "threshold = 2\n",
    "\n",
    "# Find the indices of outliers for each column\n",
    "outlier_indices = (z_scores > threshold).any(axis=1)\n",
    "\n",
    "# Extract the outlier rows from the dataset\n",
    "outliers = data[outlier_indices]\n",
    "\n",
    "# Plotting the outliers\n",
    "for column in numeric_columns:\n",
    "    plt.figure()\n",
    "    plt.boxplot(data[column])\n",
    "    plt.title(column)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2b81f77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the outlier rows from the dataset\n",
    "data_cleaned = data[~outlier_indices]\n",
    "\n",
    "# Reset the index of the cleaned dataset\n",
    "data_cleaned.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Plot histograms of the cleaned dataset\n",
    "data_cleaned[numeric_columns].hist(bins=20, figsize=(10, 8))\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot scatterplots of price against other numeric columns\n",
    "sns.pairplot(data_cleaned, x_vars=['bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot'], y_vars='price', height=5)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332ccc42",
   "metadata": {},
   "source": [
    "### One-Hot encoding\n",
    "\n",
    "First, we check for categorical columns, then get the value counts and lastly perform encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "53bfad52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking for categorical columns\n",
    "categorical_columns = data.select_dtypes(include=['object', 'category']).columns\n",
    "categorical_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a02409ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exploring categorical variables,using value_counts() to get the count of each unique value in a column \n",
    "# using describe() to get summary statistics for categorical columns.\n",
    "for column in categorical_columns:\n",
    "    value_counts = data[column].value_counts()\n",
    "    print(f\"Value counts for {column}:\\n{value_counts}\\n\")\n",
    "\n",
    "# Summary statistics for categorical columns\n",
    "print(data[categorical_columns].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9f125b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encoding to convert categoriacl values into binary'\n",
    "data_encoded = pd.get_dummies(data, columns=categorical_columns, drop_first=True)\n",
    "data_encoded.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5a456e",
   "metadata": {},
   "source": [
    " ### Feature scaling and normalization using z-scores\n",
    "This is done to numerical features to standardize their values and ensure they are on a similar scale and  helps prevent any bias or undue influence that may arise from differences in the magnitude of numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "102f51ab",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Computing the z-scores for the numerical features only\n",
    "data_numeric = data[numeric_columns]\n",
    "data_numeric = (data_numeric - data_numeric.mean()) / data_numeric.std()\n",
    "\n",
    "# Combining the encoded categorical features with the transformed numerical features\n",
    "data_encoded = pd.concat([data_encoded, data_numeric], axis=1)\n",
    "\n",
    "# Checking the updated dataset\n",
    "print(data_encoded.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6965ca33",
   "metadata": {},
   "source": [
    "### Correlation Analysis\n",
    "\n",
    "Correlation analysis is a statistical technique used to measure the strength and direction of the linear relationship between two or more variables. It helps identify the degree of association between variables and provides insights into their interdependencies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "82363df8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Correlation analysis\n",
    "correlation_matrix = data_cleaned[numeric_columns].corr()\n",
    "print(\"\\nCorrelation Matrix:\")\n",
    "print(correlation_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d5f76d",
   "metadata": {},
   "source": [
    "## Regression modelling\n",
    "\n",
    "To perform regression modeling, we will split the dataset into features (X) and target variable (y), and then split them into training and testing sets. We will use the LinearRegression model from scikit-learn library to build the regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4363f5d",
   "metadata": {},
   "source": [
    "### Baseline model\n",
    "\n",
    "We going to create a simple regression model without using any modifications to establish a baseline performance metric that other models can compare aganaist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dd7156ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting the target variable and features\n",
    "target = 'price'\n",
    "features = ['sqft_living']\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X = data[features]\n",
    "y = data[target]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create the baseline linear regression model\n",
    "baseline_model = LinearRegression()\n",
    "\n",
    "# Fit the model on the training data\n",
    "baseline_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = baseline_model.predict(X_test)\n",
    "\n",
    "# Calculate the mean squared error (MSE)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Print the MSE\n",
    "print(\"\\nBaseline Regression Model Evaluation:\")\n",
    "print(f\"R-squared: {r2:.2f}\")\n",
    "print('Baseline Model MSE:', mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1697a4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting the target variable and features\n",
    "target = 'price'\n",
    "features = ['bedrooms', 'bathrooms', 'sqft_living']\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X = data[features]\n",
    "y = data[target]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create the baseline linear regression model\n",
    "baseline_model = LinearRegression()\n",
    "\n",
    "# Fit the model on the training data\n",
    "baseline_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = baseline_model.predict(X_test)\n",
    "\n",
    "# Calculate the mean squared error (MSE)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Print the MSE\n",
    "print(\"\\nBaseline Regression Model Evaluation:\")\n",
    "print(f\"R-squared: {r2:.2f}\")\n",
    "print('Baseline Model MSE:', mse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445c254a",
   "metadata": {},
   "source": [
    "### Second Model using polynomial Transformation\n",
    "\n",
    "This model is introduced to build on the baseline model, it will incorporate additional features such as polynomial transformation to capture non linear relationships to improve the performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8f80b8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Select the target variable and features\n",
    "#X = data_encoded\n",
    "#features = ['bedrooms', 'bathrooms', 'sqft_living']\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "#X = data[features]\n",
    "#y = data[target]\n",
    "X = data_encoded.drop(['price'], axis=1)\n",
    "y = data_encoded['price']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a new feature using polynomial transformation\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "X_poly_train = poly.fit_transform(X_train)\n",
    "\n",
    "# Create a new model using the transformed features\n",
    "model_two = LinearRegression()\n",
    "model_two.fit(X_poly_train, y_train)\n",
    "\n",
    "# Transform the test features\n",
    "X_poly_test = poly.transform(X_test)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = model_two.predict(X_poly_test)\n",
    "\n",
    "# Calculate the mean squared error (MSE)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Print the MSE\n",
    "print(\"\\nModel2 Regression Model Evaluation:\")\n",
    "print(f\"R-squared: {r2:.2f}\")\n",
    "print('Model2 MSE:', mse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758a58fa",
   "metadata": {},
   "source": [
    "The improvements in R^2 indicate that the new model explains a larger portion of the variance in the target variable compared to the baseline models. This means that the new model provides a better fit to the data and has the potential to make more accurate predictions. The reduction in MSE implies that the new model has smaller prediction errors, which can be valuable for stakeholders in terms of making informed decisions or pricing properties more accurately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482892f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ded4b154",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the baseline model\n",
    "baseline_model = LinearRegression()\n",
    "baseline_model.fit(X_train, y_train)\n",
    "baseline_predictions = baseline_model.predict(X_test)\n",
    "baseline_mse = mean_squared_error(y_test, baseline_predictions)\n",
    "baseline_r2 = baseline_model.score(X_test, y_test)\n",
    "print('Baseline Model MSE:', baseline_mse)\n",
    "print('Baseline Model R^2:', baseline_r2)\n",
    "\n",
    "# Evaluate the new model\n",
    "model_two = LinearRegression()\n",
    "model_two.fit(X_train_new, y_train)\n",
    "new_predictions = model_twol.predict(X_test_new)\n",
    "new_mse = mean_squared_error(y_test, new_predictions)\n",
    "new_r2 = new_model.score(X_test_new, y_test)\n",
    "print('Model Two mse:', new_mse)\n",
    "print('Model Two R^2:', new_r2)\n",
    "\n",
    "# Justification\n",
    "if new_mse < baseline_mse:\n",
    "    print('The new model improves on the baseline model in terms of MSE.')\n",
    "else:\n",
    "    print('The new model does not provide a significant improvement over the baseline model in terms of MSE.')\n",
    "\n",
    "if new_r2 > baseline_r2:\n",
    "    print('The new model improves on the baseline model in terms of R^2.')\n",
    "else:\n",
    "    print('The new model does not provide a significant improvement over the baseline model in terms of R^2.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b480eb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Basic Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25f2792",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Polynomial Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507d66c7",
   "metadata": {},
   "source": [
    "### Regression Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467886bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the linear regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Get the coefficients\n",
    "coefficients = model.coef_\n",
    "\n",
    "# Identify the features with strong relationships to sale prices\n",
    "feature_names = X.columns\n",
    "strong_features = []\n",
    "for feature, coefficient in zip(feature_names, coefficients):\n",
    "    if abs(coefficient) > 0.5:  # Adjust the threshold as needed\n",
    "        strong_features.append((feature, coefficient))\n",
    "\n",
    "# Sort the features by their coefficients\n",
    "strong_features.sort(key=lambda x: abs(x[1]), reverse=True)\n",
    "\n",
    "# Print the strong features and their coefficients\n",
    "for feature, coefficient in strong_features:\n",
    "    print(f\"{feature}: {coefficient}\")\n",
    "\n",
    "# Explain the implications of the results\n",
    "print(\"Implications:\")\n",
    "print(\"The following features have a strong relationship with sale prices:\")\n",
    "for feature, coefficient in strong_features:\n",
    "    print(f\"- {feature}\")\n",
    "print(\"This means that these features have a significant impact on the sale prices of the properties.\")\n",
    "\n",
    "# Identify specific actions for stakeholders\n",
    "print(\"Actions for stakeholders:\")\n",
    "print(\"Based on these results, stakeholders can:\")\n",
    "print(\"- Focus on improving the quality of the properties, as 'condition_Fair' has a positive impact on sale prices.\")\n",
    "print(\"- Invest in properties with a waterfront view, as 'waterfront_YES' has a strong positive impact on sale prices.\")\n",
    "print(\"- Consider properties with larger living areas ('sqft_living'), as it has a strong positive impact on sale prices.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86657503",
   "metadata": {},
   "source": [
    "## Data Visualization\n",
    "\n",
    "Here we will be using Exploratory Data Analysis to allow shareholders understand the value and success of how different features affect prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed960d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a scatter plot between square footage and sales price\n",
    "# Create a scatter plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x='sqft_living', y='price', hue='condition', data=data_encoded)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Square Footage of Living Area')\n",
    "plt.ylabel('Sale Price')\n",
    "plt.title('Relationship between Square Footage and Sale Price')\n",
    "\n",
    "# Add a legend\n",
    "#plt.legend(title='Condition')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd192a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Create a box plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x='condition', y='price', data=data)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Condition')\n",
    "plt.ylabel('Sale Price')\n",
    "plt.title('Distribution of Sale Prices by Condition')\n",
    "\n",
    "# Add additional annotations or text if needed\n",
    "# For example, you can add median price labels\n",
    "medians = data.groupby('condition')['price'].median()\n",
    "for i, median in enumerate(medians):\n",
    "    plt.text(i, median, f'Median: ${median}', ha='center')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d614ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Calculate average sale price for each year\n",
    "avg_price_by_year = data.groupby('yr_built')['price'].mean()\n",
    "\n",
    "# Create a line plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.lineplot(x=avg_price_by_year.index, y=avg_price_by_year.values)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Year Built')\n",
    "plt.ylabel('Average Sale Price')\n",
    "plt.title('Trend of Sale Prices Over Time')\n",
    "\n",
    "# Adjust x-axis ticks and labels\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17054cae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
